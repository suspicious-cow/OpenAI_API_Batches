{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Batches\n",
    "\n",
    "The Batch API endpoint allows users to submit requests for asynchronous batch processing. We will process these requests within 24 hours. The details of each request will be read from a pre-uploaded file, and the responses will be written to an output file. You can query the batch object for status updates and results. Each model will be offered at 50% cost discount vs. the synchronous APIs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univeral Code Used for the Entire Notebook\n",
    "\n",
    "Let's set up our libraries and client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI class from the openai module\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import the time module to allow for time-related functions\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client we can use\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Batch File\n",
    "\n",
    "Batches start with a .jsonl file where each line contains the details of an individual request to the API. For now, the available endpoints are /v1/chat/completions (Chat Completions API) and /v1/embeddings (Embeddings API). For a given input file, the parameters in each line's body field are the same as the parameters for the underlying endpoint. Each request must include a unique custom_id value, which you can use to reference results after completion. Here's an example of an input file with 2 requests. Note that each input file can only include requests to a single model.\n",
    "\n",
    "NOTE: For some insane reason you are required to indicate the API endpoint here and in the batch creation later on. Presumably, OpenAI will one day allow hitting multiple different APIs in one batch request; but today is not that day. \n",
    "\n",
    "<br/>\n",
    "Examples:\n",
    "\n",
    "```\n",
    "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Give me three paragraphs on the penguin lifecycle.\"}],\"max_tokens\": 1000}}\n",
    "\n",
    "{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Give me three paragraphs on penguin mating habits.\"}],\"max_tokens\": 1000}}\n",
    "\n",
    "{\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Give me three paragraphs on penguin species differences.\"}],\"max_tokens\": 1000}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the File\n",
    "\n",
    "After creating your batch file, you must upload it so that you can reference it correctly when kicking off batches. Upload your .jsonl file using the Files API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file object for the bad batch input file and upload it using the OpenAI client\n",
    "bad_batch_input_file = client.files.create(\n",
    "    file=open(\"./artifacts/badbatchinput.jsonl\", \"rb\"),  # Open the file in read-binary mode\n",
    "    purpose=\"batch\"  # Specify the purpose of the file\n",
    ")\n",
    "\n",
    "# Create a file object for the good batch input file and upload it using the OpenAI client\n",
    "good_batch_input_file = client.files.create(\n",
    "    file=open(\"./artifacts/goodbatchinput.jsonl\", \"rb\"),  # Open the file in read-binary mode\n",
    "    purpose=\"batch\"  # Specify the purpose of the file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Batch\n",
    "\n",
    "Once you've successfully uploaded your input file, you can use the input File object's ID to create a batch. For now, the completion window can only be set to 24h. You can also provide custom metadata via an optional metadata parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_DT9jXFVLvP6UkeVZbzVBaEYg', completion_window='24h', created_at=1720349807, endpoint='/v1/chat/completions', input_file_id='file-UYzd9ehLSnVkQ6O6mUBixdXo', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1720436207, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'bad nightly penguin job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "\n",
      "\n",
      "Batch(id='batch_Maz2Rs4IBwiPHzP1O6yfyn1F', completion_window='24h', created_at=1720349808, endpoint='/v1/chat/completions', input_file_id='file-xwsJPwxz9OgOtkchtQt3uDxS', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1720436208, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'good nightly penguin job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Get the ID of the bad batch input file\n",
    "bad_batch_input_file_id = bad_batch_input_file.id\n",
    "\n",
    "# Create a batch job using the bad batch input file\n",
    "bad_batch = client.batches.create(\n",
    "    input_file_id=bad_batch_input_file_id,  # ID of the input file for the batch\n",
    "    endpoint=\"/v1/chat/completions\",  # API endpoint to use for the batch\n",
    "    completion_window=\"24h\",  # Time window for completion\n",
    "    metadata={\n",
    "        \"description\": \"bad nightly penguin job\"  # Metadata describing the batch job\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the details of the bad batch job\n",
    "print(bad_batch)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Get the ID of the good batch input file\n",
    "good_batch_input_file_id = good_batch_input_file.id\n",
    "\n",
    "# Create a batch job using the good batch input file\n",
    "good_batch = client.batches.create(\n",
    "    input_file_id=good_batch_input_file_id,  # ID of the input file for the batch\n",
    "    endpoint=\"/v1/chat/completions\",  # API endpoint to use for the batch\n",
    "    completion_window=\"24h\",  # Time window for completion\n",
    "    metadata={\n",
    "        \"description\": \"good nightly penguin job\"  # Metadata describing the batch job\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the details of the good batch job\n",
    "print(good_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Batch Status\n",
    "\n",
    "The status of a given Batch object can be any of the following:\n",
    "\n",
    "| STATUS      | DESCRIPTION                                                                  |\n",
    "|-------------|------------------------------------------------------------------------------|\n",
    "| validating  | the input file is being validated before the batch can begin                 |\n",
    "| failed      | the input file has failed the validation process                             |\n",
    "| in_progress | the input file was successfully validated and the batch is currently being run |\n",
    "| finalizing  | the batch has completed and the results are being prepared                   |\n",
    "| completed   | the batch has been completed and the results are ready                       |\n",
    "| expired     | the batch was not able to be completed within the 24-hour time window        |\n",
    "| cancelling  | the batch is being cancelled (may take up to 10 minutes)                     |\n",
    "| cancelled   | the batch was cancelled                                                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_DT9jXFVLvP6UkeVZbzVBaEYg', completion_window='24h', created_at=1720349807, endpoint='/v1/chat/completions', input_file_id='file-UYzd9ehLSnVkQ6O6mUBixdXo', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1720436207, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'bad nightly penguin job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "\n",
      "\n",
      "Batch(id='batch_Maz2Rs4IBwiPHzP1O6yfyn1F', completion_window='24h', created_at=1720349808, endpoint='/v1/chat/completions', input_file_id='file-xwsJPwxz9OgOtkchtQt3uDxS', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1720436208, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'good nightly penguin job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the details of the bad batch job\n",
    "print(client.batches.retrieve(bad_batch.id))\n",
    "print(\"\\n\\n\")  # Print newline characters for better readability\n",
    "\n",
    "# Retrieve and print the details of the good batch job\n",
    "print(client.batches.retrieve(good_batch.id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing the Batches\n",
    "\n",
    "At any time, you can see all your batches. For users with many batches, you can use the limit and after parameters to paginate your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Batch](data=[Batch(id='batch_Maz2Rs4IBwiPHzP1O6yfyn1F', completion_window='24h', created_at=1720349808, endpoint='/v1/chat/completions', input_file_id='file-xwsJPwxz9OgOtkchtQt3uDxS', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1720349816, error_file_id=None, errors=None, expired_at=None, expires_at=1720436208, failed_at=None, finalizing_at=1720349815, in_progress_at=1720349809, metadata={'description': 'good nightly penguin job'}, output_file_id='file-VpPFul3vJxByed5mL4qfvsDU', request_counts=BatchRequestCounts(completed=3, failed=0, total=3))], object='list', first_id='batch_Maz2Rs4IBwiPHzP1O6yfyn1F', last_id='batch_Maz2Rs4IBwiPHzP1O6yfyn1F', has_more=True)\n",
      "\n",
      "We have 33 batches\n",
      "\n",
      "========= ALL BATCHES ==========\n",
      "\n",
      "batch_Maz2Rs4IBwiPHzP1O6yfyn1F\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_DT9jXFVLvP6UkeVZbzVBaEYg\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_SSQ7yjhBWydbevJm6xkN6rjU\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_b78mnTOiFRDo7EHULEYN88ip\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_TzRiEi2VF0ybDqoWgj5lh0Bo\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_eO8lGLn7u8u8eR887b49cm0c\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_wo1y97JroQdOvm4kk10tjYRL\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_kSvuIWf6z3bO63Y20Q64JemH\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_dfmEaoBe0XULF3oFIY7rnxGC\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_uqEaVDcLzmYMYwgfPcvogQNw\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_rkHBlkICldIgDXZ5MaJm5xoE\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_2VJLYf7HDBsfrZsIOqex17Sh\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_LC0NkXotFC9veV5NRyIlkOjc\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_E9enNhllf055ZCFio5B0Tvjd\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_68MMFa1Z4Fq934ja1sU9UtF7\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_b4bNt66SAVJaOlvs3Iw1lJ8g\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_EU9lESgy0j3nWHqBaiuiD43m\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_2ayJiMlXB1KuLEnnsirj85eP\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_FtQXjIiZ1TQWtAEgPoYnXqwN\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_aeo2ZMmdavhEdeebQ5dNdCaz\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_digFaDnZiTa0WoPmjcwjgXwt\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_EIXIzJGPItJWrZM8eX089EQI\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_SFkOFAfJejNfQCfpK611h7UI\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_Pe9jWAK6zGIVxNSyIUMsoe5n\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_7gHdkZDI5stgn8amRIxDek6E\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_uiGGOF2Ezj2ghrbyIVxkGOXZ\n",
      "failed\n",
      "bad nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_yVmw1bKlyyTYE9GmkxRjJLBx\n",
      "completed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_iiTGl4vSFF6xL1AkGhVjOvrl\n",
      "failed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_2JxALjwCcAIoMYKBZWkkfoLs\n",
      "failed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_WS7lPOyIBe0ukFMkpmx8hbP4\n",
      "failed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_YaSbx4ToxpIZXIQg7FKTapyC\n",
      "failed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_A05MUZX2CRhuJqBywXS10gkP\n",
      "failed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_hcz7fuAGsbqu31o0p6BtjfZU\n",
      "failed\n",
      "nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "========= JUST GOOD BATCHES ==========\n",
      "\n",
      "batch_Maz2Rs4IBwiPHzP1O6yfyn1F\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_SSQ7yjhBWydbevJm6xkN6rjU\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_TzRiEi2VF0ybDqoWgj5lh0Bo\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_wo1y97JroQdOvm4kk10tjYRL\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_kSvuIWf6z3bO63Y20Q64JemH\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_uqEaVDcLzmYMYwgfPcvogQNw\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_rkHBlkICldIgDXZ5MaJm5xoE\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_LC0NkXotFC9veV5NRyIlkOjc\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_E9enNhllf055ZCFio5B0Tvjd\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_b4bNt66SAVJaOlvs3Iw1lJ8g\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_EU9lESgy0j3nWHqBaiuiD43m\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_2ayJiMlXB1KuLEnnsirj85eP\n",
      "cancelled\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_FtQXjIiZ1TQWtAEgPoYnXqwN\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_digFaDnZiTa0WoPmjcwjgXwt\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_SFkOFAfJejNfQCfpK611h7UI\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n",
      "batch_7gHdkZDI5stgn8amRIxDek6E\n",
      "completed\n",
      "good nightly penguin job\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a list of all batches\n",
    "print(client.batches.list(limit=1))\n",
    "\n",
    "# Convert the returned object to a list to get the length\n",
    "batches = list(client.batches.list())\n",
    "\n",
    "# Print the total number of batches\n",
    "print(\"\\nWe have \" + str(len(batches)) + \" batches\\n\")\n",
    "\n",
    "print(\"========= ALL BATCHES ==========\\n\")\n",
    "# Iterate over the batches and print their ids, statuses, and descriptions\n",
    "for batch in batches:\n",
    "    print(batch.id)  # Print the batch ID\n",
    "    print(batch.status)  # Print the batch status\n",
    "    print(batch.metadata.get(\"description\"))  # Print the batch description from metadata\n",
    "    print(\"\\n\\n\")  # Print newline characters for better readability\n",
    "    \n",
    "print(\"========= JUST GOOD BATCHES ==========\\n\")\n",
    "# Iterate over the good batches only and print their ids, statuses, and descriptions\n",
    "for batch in batches:\n",
    "    if batch.metadata.get(\"description\", \"\").startswith(\"good\"):\n",
    "        print(batch.id)  # Print the batch ID\n",
    "        print(batch.status)  # Print the batch status\n",
    "        print(batch.metadata.get(\"description\"))  # Print the batch description from metadata\n",
    "        print(\"\\n\\n\")  # Print newline characters for better readability\n",
    "\n",
    "\n",
    "# Do a deep dive on these at https://github.com/openai/openai-python/blob/main/src/openai/resources/batches.py\n",
    "# If you want to see the source code for the batches \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelling a Batch\n",
    "\n",
    "If necessary, you can cancel an ongoing batch. The batch's status will change to cancelling until in-flight requests are complete (up to 10 minutes), after which the status will change to cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_batch_input_file_id = good_batch_input_file.id\n",
    "\n",
    "dead_batch_walking = client.batches.create(\n",
    "    input_file_id=good_batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"good nightly penguin job\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dead_batch_walking)\n",
    "time.sleep(5)\n",
    "print(\"\\n\\n\")\n",
    "print(client.batches.retrieve(dead_batch_walking.id))\n",
    "print(\"\\n\\n\")\n",
    "print(client.batches.cancel(dead_batch_walking.id))\n",
    "time.sleep(5)\n",
    "print(\"\\n\\n\")\n",
    "print(client.batches.retrieve(dead_batch_walking.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Results\n",
    "\n",
    "Once the batch is complete, you can download the output by making a request against the Files API via the output_file_id field from the Batch object and writing it to a file on your machine, in this case batch_output.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetch the content of the file\n",
    "content = client.files.content(client.batches.retrieve(good_batch.id).output_file_id)\n",
    "\n",
    "content.write_to_file(\"./artifacts/goodbatchoutput.jsonl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
